{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 설정"
      ],
      "metadata": {
        "id": "75dClKQyOoAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.1\n",
        "# !pip install numpy==1.22.1\n",
        "# !pip install numpy==1.19.5"
      ],
      "metadata": {
        "id": "34XGgF0nPbxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5795ef-38d1-46f0-ba5e-885b51850d1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.1 in /usr/local/lib/python3.10/dist-packages (1.23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import tables\n",
        "import json\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import logging\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "import math\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DialogTransformerDataset(data.Dataset):\n",
        "    # ... (unchanged dataset class code) ...\n",
        "\n",
        "class HBertMseEuopDataset(DialogTransformerDataset):\n",
        "    # ... (unchanged dataset class code) ...\n",
        "\n",
        "def load_dict(filename):\n",
        "    return json.loads(open(filename, \"r\").readline())\n",
        "\n",
        "def load_vecs(fin):\n",
        "    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n",
        "    h5f = tables.open_file(fin)\n",
        "    h5vecs= h5f.root.vecs\n",
        "    vecs=np.zeros(shape=h5vecs.shape,dtype=h5vecs.dtype)\n",
        "    vecs[:]=h5vecs[:]\n",
        "    h5f.close()\n",
        "    return vecs\n",
        "\n",
        "def save_vecs(vecs, fout):\n",
        "    fvec = tables.open_file(fout, 'w')\n",
        "    atom = tables.Atom.from_dtype(vecs.dtype)\n",
        "    filters = tables.Filters(complib='blosc', complevel=5)\n",
        "    ds = fvec.create_carray(fvec.root,'vecs', atom, vecs.shape,filters=filters)\n",
        "    ds[:] = vecs\n",
        "    print('done')\n",
        "    fvec.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # File and tokenizer setup\n",
        "    input_dir = './data/reddit/'\n",
        "    VALID_FILE = input_dir + 'train.h5'\n",
        "    task = 'test_ctx'  # or 'test_utt'\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    if task == 'test_utt':\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, utt_masklm=True, utt_sop=True)\n",
        "    elif task == 'test_ctx':\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, context_shuf=True, context_masklm=False)\n",
        "    else:\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "    eval_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=1)\n",
        "\n",
        "    # Model setup\n",
        "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    learning_rate = 5e-5\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = CrossEntropyLoss()\n",
        "\n",
        "    # Training settings\n",
        "    epochs = 3\n",
        "    history = {'train_loss': [], 'eval_loss': [], 'train_accuracy': [], 'eval_accuracy': [], 'train_perplexity': [], 'eval_perplexity': []}\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            optimizer.zero_grad()\n",
        "            context, context_utts_attn_mask, context_attn_mask, context_mlm_target, context_position_perm_id, context_position_ids, response = batch\n",
        "\n",
        "            input_ids = context.view(-1, context.size(-1)).to(device)\n",
        "            attention_mask = context_attn_mask.view(-1, context_attn_mask.size(-1)).to(device)\n",
        "            labels = context_mlm_target.view(-1, context_mlm_target.size(-1)).to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_predictions += labels.numel()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = correct_predictions / total_predictions\n",
        "        train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_accuracy'].append(train_accuracy)\n",
        "        history['train_perplexity'].append(train_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
        "\n",
        "        # Evaluation loop\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        eval_predictions = []\n",
        "        eval_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(eval_loader, desc=f\"Evaluation Epoch {epoch+1}\"):\n",
        "                context, context_utts_attn_mask, context_attn_mask, context_mlm_target, context_position_perm_id, context_position_ids, response = batch\n",
        "\n",
        "                input_ids = context.view(-1, context.size(-1)).to(device)\n",
        "                attention_mask = context_attn_mask.view(-1, context_attn_mask.size(-1)).to(device)\n",
        "                labels = context_mlm_target.view(-1, context_mlm_target.size(-1)).to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                eval_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                correct_predictions += (preds == labels).sum().item()\n",
        "                total_predictions += labels.numel()\n",
        "\n",
        "                eval_predictions.extend(preds.cpu().numpy())\n",
        "                eval_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        eval_accuracy = correct_predictions / total_predictions\n",
        "        eval_perplexity = math.exp(avg_eval_loss)\n",
        "\n",
        "        history['eval_loss'].append(avg_eval_loss)\n",
        "        history['eval_accuracy'].append(eval_accuracy)\n",
        "        history['eval_perplexity'].append(eval_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Eval Loss: {avg_eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}, Eval Perplexity: {eval_perplexity:.4f}\")\n",
        "\n",
        "        all_predictions.extend(eval_predictions)\n",
        "        all_targets.extend(eval_targets)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Ysa7kv-YsXCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NW4bCqvQ4du6",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe17819-df2c-45af-ec65-1ca972f3f75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.2.2)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.3\n",
            "    Uninstalling graphviz-0.20.3:\n",
            "      Successfully uninstalled graphviz-0.20.3\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
            "Collecting gluonnlp==0.8.0\n",
            "  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.1)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292696 sha256=6f3b683d028de7186cc43d46db8137633af10a864f87b8a510ad379754d91fe0\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.8.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch>=1.8.1\n",
        "!pip install tqdm\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAjup-s_O_Nx",
        "outputId": "ceb4a779-1efb-41ec-c718-b880796738d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-k48gm03h/kobert-tokenizer_5569dc6d20294700a7056b95706fb599\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-k48gm03h/kobert-tokenizer_5569dc6d20294700a7056b95706fb599\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kobert_tokenizer\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=4aaae081534fd244774d8f8af1c10cdae38b19a86a9bd0b93f10a99cccb212bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jppffb5r/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built kobert_tokenizer\n",
            "Installing collected packages: kobert_tokenizer\n",
            "Successfully installed kobert_tokenizer-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZatSfiCe2V4",
        "outputId": "4b206e6c-682b-4d39-bd90-52ea817150cd",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.3.0+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=45d1fbcd1dcdb5b43197082f0c17c7503b45e2c7432b1564b0c0b4369015be69\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce45bd2d-e066-474a-fb91-4efcd083fe3f",
        "id": "u32pNGbadmPc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "x3qDySlMziCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import MSELoss, CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchsummary import summary\n",
        "from torchviz import make_dot\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertForMaskedLM, BertConfig, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uKb5GENeBtM8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토크나이저"
      ],
      "metadata": {
        "id": "0NBec-69B4aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT 토크나이저 불러오기\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzRjBMVFByKG",
        "outputId": "a0fa950f-3710-4b0b-9855-c65c2cd43edc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 & 데이터로더 정의"
      ],
      "metadata": {
        "id": "bafmQxYSB6Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리된 데이터셋 파일 불러오기"
      ],
      "metadata": {
        "id": "sXtvba6YXuLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/NLP/Dataset/240527'\n",
        "file_name = '[haein]dataset_cleansing+koSpacing+drop_duplicate_240527.csv'\n",
        "\n",
        "df = pd.read_csv(folder_path+'/'+file_name)\n",
        "df"
      ],
      "metadata": {
        "id": "5EmRvka7Xtoj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "dcace171-7a66-4217-bd8e-006152bb1ebf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              id  sort  NO1  \\\n",
              "0      2000001.0     1  0.0   \n",
              "1      2000001.0     2  0.0   \n",
              "2      2000001.0     3  0.0   \n",
              "3      2000001.0     4  0.0   \n",
              "4      2000001.0     5  0.0   \n",
              "...          ...   ...  ...   \n",
              "48049     8727.0    15  0.0   \n",
              "48050     8727.0    16  0.0   \n",
              "48051     8727.0    17  0.0   \n",
              "48052     8727.0    18  0.0   \n",
              "48053     8727.0    19  0.0   \n",
              "\n",
              "                                                  column  내담자  상담자  token_len  \\\n",
              "0                        네, 안녕하세요. 요즘 정말 좋은 일이 많이 있었거든요.  1.0  0.0         16   \n",
              "1                            네, 맞아요. 정말 행복한 시기를 보내고 있어요.  1.0  0.0         15   \n",
              "2           그렇죠. 어려운 시기를 견뎌내고 이렇게 행복한 날들을 맞이할 수 있어 감사해요.  1.0  0.0         25   \n",
              "3                                    맞아요. 정말 기분 좋은 일이에요.  1.0  0.0         10   \n",
              "4                             네, 감사합니다. 상담해주셔서 정말 고맙습니다.  1.0  0.0         15   \n",
              "...                                                  ...  ...  ...        ...   \n",
              "48049   그리고 저희는 더 자세하게 이야기를 나누어봐야겠지만, 가족들이 함께 겪는 이런 힘...  0.0  1.0         34   \n",
              "48050                            아.. 그렇게 이해해야겠어요. 고맙습니다.  1.0  0.0         13   \n",
              "48051   더욱 구체적인 이야기를 나누어봐야겠지만, 지금처럼 서로 이해하고, 공감해주는 마음...  0.0  1.0         38   \n",
              "48052   네, 이번 상담으로 좀 더 지혜롭게 가족과 지낼 수 있는 방법을 찾을 수 있을 것...  1.0  0.0         25   \n",
              "48053   좋습니다. 님께서는 이번 상황에서 님 본인의 감정과 엄마의 감정에 대해서 적극적으...  0.0  1.0         46   \n",
              "\n",
              "                                        corrected_column  \n",
              "0                         네, 안녕하세요.요즘 정말 좋은 일이 많이 있었거든요.  \n",
              "1                             네, 맞아요.정말 행복한 시기를 보내고 있어요.  \n",
              "2           그렇죠. 어려운 시기를 견뎌내고 이렇게 행복한 날들을 맞이할 수 있어 감사해요.  \n",
              "3                                     맞아요.정말 기분 좋은 일이에요.  \n",
              "4                             네, 감사합니다. 상담해주셔서 정말 고맙습니다.  \n",
              "...                                                  ...  \n",
              "48049  그리고 저희는 더 자세하게 이야기를 나누어봐야겠지만, 가족들이 함께 겪는 이런 힘든...  \n",
              "48050                            아.. 그렇게 이해해야겠어요. 고맙습니다.  \n",
              "48051  더욱 구체적인 이야기를 나누어봐야겠지만, 지금처럼 서로 이해하고, 공감해주는 마음을...  \n",
              "48052  네, 이번 상담으로 좀 더 지혜롭게 가족과 지낼 수 있는 방법을 찾을 수 있을 것 ...  \n",
              "48053  좋습니다.님께서는 이번 상황에서 님 본인의 감정과 엄마의 감정에 대해서 적극적으로 ...  \n",
              "\n",
              "[48054 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ceb43b8-d091-4544-8e92-4aa37cf3a650\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sort</th>\n",
              "      <th>NO1</th>\n",
              "      <th>column</th>\n",
              "      <th>내담자</th>\n",
              "      <th>상담자</th>\n",
              "      <th>token_len</th>\n",
              "      <th>corrected_column</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000001.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>네, 안녕하세요. 요즘 정말 좋은 일이 많이 있었거든요.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16</td>\n",
              "      <td>네, 안녕하세요.요즘 정말 좋은 일이 많이 있었거든요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000001.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>네, 맞아요. 정말 행복한 시기를 보내고 있어요.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15</td>\n",
              "      <td>네, 맞아요.정말 행복한 시기를 보내고 있어요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000001.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>그렇죠. 어려운 시기를 견뎌내고 이렇게 행복한 날들을 맞이할 수 있어 감사해요.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25</td>\n",
              "      <td>그렇죠. 어려운 시기를 견뎌내고 이렇게 행복한 날들을 맞이할 수 있어 감사해요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000001.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>맞아요. 정말 기분 좋은 일이에요.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>맞아요.정말 기분 좋은 일이에요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000001.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>네, 감사합니다. 상담해주셔서 정말 고맙습니다.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15</td>\n",
              "      <td>네, 감사합니다. 상담해주셔서 정말 고맙습니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48049</th>\n",
              "      <td>8727.0</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>그리고 저희는 더 자세하게 이야기를 나누어봐야겠지만, 가족들이 함께 겪는 이런 힘...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>34</td>\n",
              "      <td>그리고 저희는 더 자세하게 이야기를 나누어봐야겠지만, 가족들이 함께 겪는 이런 힘든...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48050</th>\n",
              "      <td>8727.0</td>\n",
              "      <td>16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>아.. 그렇게 이해해야겠어요. 고맙습니다.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13</td>\n",
              "      <td>아.. 그렇게 이해해야겠어요. 고맙습니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48051</th>\n",
              "      <td>8727.0</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>더욱 구체적인 이야기를 나누어봐야겠지만, 지금처럼 서로 이해하고, 공감해주는 마음...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38</td>\n",
              "      <td>더욱 구체적인 이야기를 나누어봐야겠지만, 지금처럼 서로 이해하고, 공감해주는 마음을...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48052</th>\n",
              "      <td>8727.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>네, 이번 상담으로 좀 더 지혜롭게 가족과 지낼 수 있는 방법을 찾을 수 있을 것...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25</td>\n",
              "      <td>네, 이번 상담으로 좀 더 지혜롭게 가족과 지낼 수 있는 방법을 찾을 수 있을 것 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48053</th>\n",
              "      <td>8727.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>좋습니다. 님께서는 이번 상황에서 님 본인의 감정과 엄마의 감정에 대해서 적극적으...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>46</td>\n",
              "      <td>좋습니다.님께서는 이번 상황에서 님 본인의 감정과 엄마의 감정에 대해서 적극적으로 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48054 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ceb43b8-d091-4544-8e92-4aa37cf3a650')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ceb43b8-d091-4544-8e92-4aa37cf3a650 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ceb43b8-d091-4544-8e92-4aa37cf3a650');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e117d7fd-825d-40cc-b426-49d3387ae215\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e117d7fd-825d-40cc-b426-49d3387ae215')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e117d7fd-825d-40cc-b426-49d3387ae215 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2e5dae65-982a-4e98-826a-2763ac3387f8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2e5dae65-982a-4e98-826a-2763ac3387f8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 48054,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 783130.8970882464,\n        \"min\": 1.0,\n        \"max\": 3000383.0,\n        \"num_unique_values\": 3520,\n        \"samples\": [\n          5250.0,\n          1000887.0,\n          3000055.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sort\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 84,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          74,\n          1,\n          59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NO1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17206359912229072,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"column\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 46232,\n        \"samples\": [\n          \" \\ub9e4\\uc77c \\uac15\\uc81c\\ub85c \\uc77c\\ud558\\ub294 \\uac83 \\uac19\\uc544\\uc11c \\uc815\\ub9d0 \\ub108\\ubb34 \\ud798\\ub4dc\\ub124\\uc694. \\uc77c\\uc744 \\ud574\\ub098\\uac00\\uba70 \\ud798\\ub4e4\\uace0 \\uc9c0\\uce58\\uba74\\uc11c \\uc77c\\uc758 \\uac00\\uce58\\ub97c \\ub290\\ub07c\\uae30\\ub3c4 \\uc5b4\\ub835\\uace0, \\ucc38\\uae30 \\ud798\\ub4e4\\uc5b4\\uc84c\\uc5b4\\uc694.\",\n          \" \\uc800 \\uc774\\uc81c \\uc9c1\\uc7a5\\uc778\\uc778\\ub370, \\ud1f4\\uadfc\\ud558\\uba74 \\uc9d1\\uc5d0\\uc11c \\ud55c\\uac00\\ud558\\uac8c \\uc788\\uae30\\uac00 \\uc2eb\\uc5b4\\uc694.\",\n          \" \\uadf8\\ub0e5 \\uc2dc\\uac04\\ub3c4 \\uc9c0\\ub098\\uace0, \\uc77c\\ud560 \\uc218 \\uc788\\ub294 \\uacf3\\uc744 \\ucc3e\\ub2e4\\uac00 \\uc774 \\ud68c\\uc0ac\\uc5d0 \\uc9c0\\uc6d0\\ud588\\uc5b4\\uc694. \\uadf8\\ub9ac\\uace0 \\uba74\\uc811\\uc5d0\\uc11c\\ub3c4 \\uc774 \\uc77c\\uc5d0 \\ub300\\ud574\\uc11c \\ubb3c\\uc5b4\\ubd24\\ub294\\ub370, \\ud2b9\\ubcc4\\ud788 \\uad00\\uc2ec\\uc774 \\uc5c6\\ub2e4\\ub294 \\ub2f5\\ubcc0\\uc744 \\ub4e4\\uc5c8\\uc5b4\\uc694. \\uadf8\\ub798\\uc11c \\uc774 \\uc77c\\uc744 \\ubc1b\\uac8c \\ub418\\uc5c8\\uc8e0.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ub0b4\\ub2f4\\uc790\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49957886463507417,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc0c1\\ub2f4\\uc790\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49957886463507417,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33,\n        \"min\": 1,\n        \"max\": 301,\n        \"num_unique_values\": 246,\n        \"samples\": [\n          109,\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"corrected_column\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 46156,\n        \"samples\": [\n          \"\\uc5b4\\ub5a4 \\uc77c\\uc774 \\uc788\\uc5c8\\ub294\\uc9c0 \\uc880 \\ub354 \\uad6c\\uccb4\\uc801\\uc73c\\ub85c \\ub9d0\\uc500\\ud574 \\uc8fc\\uc2e4 \\uc218 \\uc788\\ub098\\uc694?\",\n          \"\\uc7ac\\ubbf8\\uc788\\ub294 \\uc77c\\uc744 \\ucc3e\\ub294 \\uac78\\uae4c\\uc694?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "RxgCv4dBdNut"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF 벡터화\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=5)  # 상위 1000개의 단어로 제한\n",
        "tfidf_vectorizer.fit(df['column'])  # 전체 데이터의 텍스트를 사용하여 벡터라이저를 학습시킴\n",
        "\n",
        "# 상위 1000개의 단어 출력\n",
        "top_1000_words = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"TF-IDF 상위 1000개 단어:\", top_1000_words)\n",
        "\n",
        "# 원본 텍스트에서 TF-IDF 상위 1000개 단어만 남기기\n",
        "def filter_text_by_top_words(text, top_words):\n",
        "    filtered_words = [word for word in text.split() if word in top_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['filtered_text'] = df['column'].apply(lambda x: filter_text_by_top_words(x, top_1000_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr439TI9dKFW",
        "outputId": "38bbe8da-28fa-4e73-b0fc-f827e4787efc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF 상위 1000개 단어: ['가끔은' '가는' '가능성이' '가면' '가장' '가족' '가족과' '가족들과' '가족들이' '가지' '가지고' '가지는'\n",
            " '가진' '가질' '가치를' '간단한' '갈등이' '감사합니다' '감사해요' '감정에' '감정을' '감정이' '갑자기' '갖고'\n",
            " '갖는' '같고' '같네요' '같다는' '같습니다' '같아' '같아서' '같아서요' '같아요' '같은' '같은데' '같이' '거라'\n",
            " '거라고' '거에요' '거예요' '거의' '거죠' '걱정을' '걱정이' '건가요' '건강한' '건지' '걸까요' '것도' '것들을'\n",
            " '것들이' '것만으로도' '것보다' '것부터' '것에' '것으로' '것은' '것을' '것이' '것인지' '것입니다' '것처럼'\n",
            " '겪고' '결과가' '결과를' '결국' '결혼을' '경우' '경우가' '경우도' '경우에는' '경험을' '경험이' '계속'\n",
            " '계속해서' '계시군요' '계시나요' '계시는' '계신' '계신가요' '계획을' '고마워요' '고맙습니다' '고민으로' '고민을'\n",
            " '고민이' '고민하고' '고통을' '공부를' '과정에서' '관계가' '관계를' '관련된' '관심을' '괜찮은' '굉장히'\n",
            " '구체적으로' '궁금한' '규칙적인' '균형을' '그건' '그걸' '그것도' '그것은' '그것을' '그것이' '그게' '그냥'\n",
            " '그동안' '그래도' '그래서' '그래요' '그러게요' '그러나' '그러니까' '그러면' '그러면서' '그러셨군요' '그러시군요'\n",
            " '그러한' '그런' '그런데' '그럴' '그럼' '그렇게' '그렇군요' '그렇다면' '그렇습니다' '그렇지' '그리고' '그만큼'\n",
            " '그에' '그저' '극복할' '근데' '긍정적으로' '긍정적인' '기분을' '기분이' '기회가' '기회를' '깊이' '꾸준히'\n",
            " '나가는' '나갈' '나누고' '나누는' '나누면서' '나누어' '나눌' '나는' '나를' '나쁜' '나서' '나아갈' '나아질'\n",
            " '나은' '나의' '내가' '내에서' '내용을' '내용이' '너무' '노력을' '노력이' '노력하고' '노력하는' '노력해보세요'\n",
            " '높이는' '높일' '누구나' '느껴져요' '느껴지는' '느끼게' '느끼고' '느끼는' '느끼시나요' '느끼시는' '느끼실'\n",
            " '느낄' '느낌을' '느낌이' '능력을' '능력이' '님께서' '님께서는' '님은' '님을' '님의' '님이' '다르게' '다른'\n",
            " '다시' '다양한' '다음' '다음에' '당신에게' '당신은' '당신을' '당신의' '당신이' '당연한' '당연히' '당장'\n",
            " '대로' '대처' '대처하는' '대처할' '대처해야' '대한' '대해' '대해서' '대해서는' '대해서도' '대화' '대화가'\n",
            " '대화를' '대화에서' '대화하는' '대화할' '더욱' '도와드릴' '도와드릴게요' '도와드릴까요' '도움을' '도움이' '돈을'\n",
            " '동료들과' '동료들이' '동안' '돼요' '됐어요' '되고' '되네요' '되는' '되면' '되면서' '되어' '되었습니다'\n",
            " '되었어요' '되지' '된다는' '된다면' '될까요' '됩니다' '드는' '드릴' '드시나요' '듣고' '듣는' '들고' '들면'\n",
            " '들어' '들어서' '들어요' '등을' '등의' '등이' '따라' '따라서' '때가' '때는' '때도' '때로는' '때마다'\n",
            " '때문에' '때문입니다' '때부터' '때에는' '떨어지고' '떨어지는' '또한' '라는' '마세요' '마시고' '마음' '마음도'\n",
            " '마음에' '마음으로' '마음을' '마음의' '마음이' '마지막으로' '만나는' '만드는' '만들어' '만약' '만큼' '만한'\n",
            " '많고' '많습니다' '많아서' '많아요' '많아져서' '많아지면서' '많은' '많이' '말고' '말도' '말씀' '말씀을'\n",
            " '말씀하신' '말씀해' '말씀해주세요' '말씀해주시겠어요' '말씀해주신' '말씀해주실' '말을' '말이' '말하는' '맞는'\n",
            " '맞습니다' '맞아요' '맞지' '맡게' '맡은' '매우' '매일' '머리가' '먼저' '모두' '모든' '모르겠습니다'\n",
            " '모르겠어요' '모습을' '목표를' '몰라서' '몸과' '못하게' '못하고' '못하는' '못하면' '못한' '못할' '못해서'\n",
            " '무슨' '무엇을' '무엇이' '무엇인가요' '무엇인지' '문제' '문제가' '문제는' '문제로' '문제를' '문제에' '물론'\n",
            " '뭐가' '뭔가' '미리' '바랍니다' '바로' '바탕으로' '반드시' '받게' '받고' '받는' '받아보는' '받아서' '받아요'\n",
            " '받으러' '받으면' '받으시는' '받을' '받지' '발생할' '방법' '방법도' '방법들을' '방법들이' '방법에' '방법으로'\n",
            " '방법으로는' '방법은' '방법을' '방법이' '방법입니다' '방식으로' '방향으로' '배울' '변화가' '변화를' '별로'\n",
            " '보고' '보내고' '보는' '보니' '보면' '보세요' '보시는' '본인의' '본인이' '볼게요' '봐요' '부담을' '부담이'\n",
            " '부모님과' '부모님께' '부모님의' '부모님이' '부분에' '부분에서' '부분은' '부분을' '부분이' '부정적인' '부족한'\n",
            " '분들이' '분명' '분야에서' '불안감과' '불안감을' '불안감이' '불안하고' '불안한' '불편한' '불필요한' '사는'\n",
            " '사람과' '사람들과' '사람들에게' '사람들은' '사람들을' '사람들의' '사람들이' '사람은' '사람을' '사람이' '사실'\n",
            " '살고' '살아가는' '살아갈' '살아야' '삶에' '삶을' '삶의' '삶이' '상담' '상담사' '상담사님' '상담사입니다'\n",
            " '상담에' '상담을' '상담이' '상당히' '상대방과' '상대방에게' '상대방의' '상대방이' '상사가' '상사와' '상처를'\n",
            " '상황에' '상황에서' '상황에서는' '상황에서도' '상황은' '상황을' '상황이' '새로운' '생각' '생각과' '생각도'\n",
            " '생각에' '생각은' '생각을' '생각이' '생각하게' '생각하고' '생각하는' '생각하면' '생각하시나요' '생각하시는'\n",
            " '생각합니다' '생각해' '생각해보는' '생각해보면' '생각해보세요' '생각해보시는' '생각해요' '생기는' '생기면' '생길'\n",
            " '생활' '생활을' '생활의' '서로' '서로를' '서로에게' '서로의' '선생님' '선택을' '성과를' '성취감을' '소중한'\n",
            " '소통이' '소통하는' '솔직하게' '수가' '수도' '수면' '쉬는' '쉬운' '쉽게' '쉽지' '스스로' '스스로를'\n",
            " '스스로에게' '스트레스' '스트레스가' '스트레스는' '스트레스도' '스트레스를' '스트레스와' '습관을' '시간' '시간도'\n",
            " '시간에' '시간을' '시간이' '시작하는' '식으로' '실수를' '실제로' '싫어요' '심리상담사입니다' '심리상담을' '싶다는'\n",
            " '싶습니다' '싶어서' '싶어요' '싶은' '싶은데' '싶지' '아니라' '아니에요' '아니요' '아닌' '아닙니다' '아무'\n",
            " '아무것도' '아무리' '아직' '안녕하세요' '안되고' '안되는' '않고' '않는' '않도록' '않습니다' '않아' '않아도'\n",
            " '않아서' '않아요' '않으면' '않은' '않을' '않을까' '않을까요' '알겠습니다' '알고' '앞으로' '앞으로도' '얘기를'\n",
            " '어느' '어떠세요' '어떤' '어떨까요' '어떻게' '어려운' '어려울' '어려움을' '어려움이' '어려워요' '어렵게'\n",
            " '어렵고' '어렵다면' '어렵습니다' '어머니와' '언제나' '언제든' '언제든지' '언제부터' '얻을' '얼마나' '엄마가'\n",
            " '업무' '업무가' '업무량이' '업무를' '업무에' '업무와' '업무의' '없고' '없는' '없다는' '없습니다' '없어서'\n",
            " '없어요' '없을' '없을까요' '없이' '여기' '여기서' '여러' '여러분의' '여유를' '여전히' '역량을' '역할을'\n",
            " '연락을' '연락주세요' '열심히' '영향을' '예를' '오늘' '오늘은' '오셨나요' '오히려' '왔습니다' '요즘' '요즘은'\n",
            " '우리' '우리가' '우리는' '우선' '우선순위를' '우선은' '우울증' '운동' '운동을' '운동이나' '원인을' '원인이'\n",
            " '원하는' '위한' '위해' '위해서' '위해서는' '유지하는' '의견을' '의미가' '의미를' '이것은' '이것이' '이게'\n",
            " '이겨낼' '이는' '이대로' '이러한' '이런' '이럴' '이렇게' '이를' '이미' '이번' '이번에' '이상' '이상하게'\n",
            " '이상한' '이야기' '이야기를' '이야기하는' '이야기할' '이야기해' '이에' '이유가' '이유는' '이유를' '이전'\n",
            " '이전에' '이제' '이제는' '이제부터' '이젠' '이직을' '이해가' '이해를' '이해하고' '이해하는' '이해하지' '이해할'\n",
            " '이해합니다' '이후에도' '인해' '일과' '일단' '일도' '일들을' '일들이' '일로' '일만' '일상생활에서' '일상에서'\n",
            " '일상적인' '일어나는' '일에' '일은' '일을' '일의' '일이' '일이나' '일이에요' '일입니다' '일정을' '일하고'\n",
            " '일하는' '일하면서' '일할' '입니다' '있게' '있겠네요' '있겠죠' '있겠지만' '있고' '있기' '있나요' '있는'\n",
            " '있는데' '있는지' '있다고' '있다는' '있다면' '있도록' '있습니다' '있어서' '있어요' '있었나요' '있었는데'\n",
            " '있으니' '있으며' '있으면' '있으시군요' '있으신' '있으신가요' '있을' '있을까요' '있을지' '있죠' '있지' '있지만'\n",
            " '잊지' '자기' '자꾸' '자살' '자세히' '자신감도' '자신감을' '자신감이' '자신만의' '자신에' '자신에게' '자신을'\n",
            " '자신의' '자신이' '자연스럽게' '자유롭게' '자존감이' '자주' '자체가' '작은' '잘못된' '잘못한' '잘하고' '잘하는'\n",
            " '잘할' '잠도' '잠을' '저는' '저도' '저를' '저만' '저에게' '저와' '저의' '저희' '저희가' '적극적으로'\n",
            " '적극적인' '적이' '적절한' '전문가의' '전문적인' '전에' '전혀' '절대' '점이' '점점' '정말' '정말로' '정보를'\n",
            " '정신적으로' '정하고' '정확히' '제가' '제게' '제대로' '제일' '조금' '조금씩' '조금은' '조금이나마' '조금이라도'\n",
            " '조언' '조언을' '존중하는' '좋겠네요' '좋겠습니다' '좋겠어요' '좋겠죠' '좋습니다' '좋아요' '좋아하는' '좋은'\n",
            " '좋을' '좋을까요' '좋을지' '좋지' '주는' '주변' '주변에' '주세요' '주셔서' '주실' '죽고' '준비를' '줄이기'\n",
            " '줄이는' '줄일' '중에' '중요한' '중요합니다' '중요해요' '증상이' '지금' '지금까지' '지금부터' '지금은' '지금의'\n",
            " '지금처럼' '지내고' '지속적으로' '지쳐서' '지치고' '지친' '직장' '직장에서' '직접' '진짜' '질문을' '집에'\n",
            " '집에서' '집중력을' '집중력이' '집중이' '차근차근' '차라리' '찾고' '찾는' '찾아보는' '찾아보면' '찾아보세요'\n",
            " '찾아보시는' '찾아서' '찾아주세요' '찾을' '책을' '처리하는' '처리할' '처음' '처음에는' '천천히' '최근에'\n",
            " '최대한' '최선을' '충분한' '충분히' '취미' '취미나' '취미를' '취미생활을' '취미활동을' '취하는' '친구' '친구가'\n",
            " '친구들과' '친구들은' '친구들이' '친구를' '친구와' '크게' '텐데' '통해' '통해서' '특히' '파악하고' '편안하게'\n",
            " '편하게' '포기하지' '표현하는' '푸는' '필요가' '필요한' '필요할' '필요합니다' '하거나' '하게' '하겠습니다'\n",
            " '하고' '하기' '하나' '하나씩' '하나요' '하나의' '하나입니다' '하는' '하는데' '하는지' '하다가' '하다보면'\n",
            " '하려고' '하루' '하루종일' '하며' '하면' '하면서' '하셨는데' '하시나요' '하시는' '하시면' '하시면서' '하죠'\n",
            " '하지' '하지만' '학교' '학교에' '학교에서' '한다는' '한번' '할까요' '할지' '함께' '함께하는' '합니다' '항상'\n",
            " '해결' '해결책을' '해결하기' '해결하는' '해결할' '해결해' '해결해야' '해도' '해보겠습니다' '해보고' '해보는'\n",
            " '해보면' '해보세요' '해볼게요' '해봐야겠어요' '해서' '해소하는' '해소할' '해야' '해야할' '해야할까요' '해야할지'\n",
            " '해요' '했는데' '했어요' '행동을' '행복한' '현재' '현재의' '혹시' '혼자' '혼자가' '혼자서' '화가' '환경에서'\n",
            " '활동을' '회사' '회사에' '회사에서' '회사에서는' '효율적으로' '휴식을' '희망을' '힘드셨겠어요' '힘드시겠어요'\n",
            " '힘드신' '힘든' '힘들' '힘들게' '힘들고' '힘들어서' '힘들어요' '힘들어지고' '힘을' '힘이']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터에 NaN 값이 있는지 확인\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbLhnRJ-op-q",
        "outputId": "20e4df8b-470b-422d-cd51-2133f5323e8b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                  0\n",
            "sort                0\n",
            "NO1                 0\n",
            "column              0\n",
            "내담자                 0\n",
            "상담자                 0\n",
            "token_len           0\n",
            "corrected_column    0\n",
            "filtered_text       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 학습용과 평가용으로 분리\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# 데이터프레임 확인\n",
        "print(train_df[['column', 'filtered_text']].head())"
      ],
      "metadata": {
        "id": "LTFBxuRvomvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85efb89-5b51-4efe-fa8e-59d471edbec1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  column  \\\n",
            "34920                          그렇다면 어떤 점이 가장 힘들게 느껴지시나요?   \n",
            "1661    지금 바로 희망이 보이지 않는 건 당연해요. 그래도 조금씩, 천천히 나아가다 보면...   \n",
            "10894   우선, 대인관계에서 불안감을 느끼시면 그걸 이해하고 수용할 수 있는 관점을 갖는 ...   \n",
            "29042   잘 이해합니다. 기분 나쁜 일을 당하셨군요. 혹시 인간관계가 문제가 된 이유를 파...   \n",
            "3734    그럴 때마다 정말 힘드시겠어요. 스트레스가 많아서 더 힘들어지는 것 같네요. 어떤...   \n",
            "\n",
            "                                           filtered_text  \n",
            "34920                                  그렇다면 어떤 점이 가장 힘들게  \n",
            "1661   지금 바로 않는 그래도 천천히 보면 어느 삶의 의미를 당신은 정말 소중한 잊지 주변...  \n",
            "10894  불안감을 그걸 이해하고 있는 갖는 것이 상대방의 상대방과 자신의 의견을 표현하는 것...  \n",
            "29042                                   나쁜 일을 혹시 문제가 이유를  \n",
            "3734   그럴 때마다 정말 스트레스가 많아서 어떤 것들이 스트레스를 그리고 어떤 상황에서 가...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape, eval_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shh32_vpdpGO",
        "outputId": "89acad24-dbd5-49f6-977c-da69334eda91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(43248, 9) (4806, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5OQF_1Td1ct",
        "outputId": "97e550e8-416d-47ff-9815-3bcc8849a271"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              id  sort  NO1  \\\n",
            "34920     5583.0     9  0.0   \n",
            "1661   1001006.0    11  0.0   \n",
            "10894      500.0    15  0.0   \n",
            "29042     4297.0     3  0.0   \n",
            "3734   3000082.0     9  0.0   \n",
            "\n",
            "                                                  column  내담자  상담자  token_len  \\\n",
            "34920                          그렇다면 어떤 점이 가장 힘들게 느껴지시나요?  0.0  1.0         14   \n",
            "1661    지금 바로 희망이 보이지 않는 건 당연해요. 그래도 조금씩, 천천히 나아가다 보면...  0.0  1.0         93   \n",
            "10894   우선, 대인관계에서 불안감을 느끼시면 그걸 이해하고 수용할 수 있는 관점을 갖는 ...  0.0  1.0         87   \n",
            "29042   잘 이해합니다. 기분 나쁜 일을 당하셨군요. 혹시 인간관계가 문제가 된 이유를 파...  0.0  1.0         28   \n",
            "3734    그럴 때마다 정말 힘드시겠어요. 스트레스가 많아서 더 힘들어지는 것 같네요. 어떤...  0.0  1.0         46   \n",
            "\n",
            "                                        corrected_column  \\\n",
            "34920                         그렇다면 어떤 점이 가장 힘들게 느껴 지시나요?   \n",
            "1661   지금 바로 희망이 보이지 않는 건 당연해요. 그래도 조금씩, 천천히 나아가다 보면 ...   \n",
            "10894  우선, 대인관계에서 불안감을 느끼시면 그걸 이해하고 수용할 수 있는 관점을 갖는 것...   \n",
            "29042  잘 이해합니다.기분 나쁜 일을 당하셨군요. 혹시 인간관계가 문제가 된 이유를 파악하...   \n",
            "3734   그럴 때마다 정말 힘드시겠어요. 스트레스가 많아서 더 힘들어지는 것 같네요. 어떤 ...   \n",
            "\n",
            "                                           filtered_text  \n",
            "34920                                  그렇다면 어떤 점이 가장 힘들게  \n",
            "1661   지금 바로 않는 그래도 천천히 보면 어느 삶의 의미를 당신은 정말 소중한 잊지 주변...  \n",
            "10894  불안감을 그걸 이해하고 있는 갖는 것이 상대방의 상대방과 자신의 의견을 표현하는 것...  \n",
            "29042                                   나쁜 일을 혹시 문제가 이유를  \n",
            "3734   그럴 때마다 정말 스트레스가 많아서 어떤 것들이 스트레스를 그리고 어떤 상황에서 가...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import tables\n",
        "import json\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import logging\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "import math\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DialogTransformerDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    A base class for Transformer dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, tokenizer,\n",
        "                 min_num_utts=1, max_num_utts=7, max_utt_len=30,\n",
        "                 block_size=256, utt_masklm=False, utt_sop=False,\n",
        "                 context_shuf=False, context_masklm=False):\n",
        "        # 1. Initialize file path or list of file names.\n",
        "        \"\"\"read training sentences(list of int array) from a hdf5 file\"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.min_num_utts = min_num_utts #if not context_shuf and not context_masklm else 3\n",
        "        self.max_num_utts = max_num_utts\n",
        "        self.max_utt_len =max_utt_len\n",
        "        self.block_size = block_size # segment size to train BERT. when set -1 by default, use indivicual sentences(responses) as BERT inputs.\n",
        "                            # Otherwise, clip a block from the context.\n",
        "\n",
        "        self.utt_masklm = utt_masklm\n",
        "        self.utt_sop =utt_sop\n",
        "        self.context_shuf =context_shuf\n",
        "        self.context_masklm =context_masklm\n",
        "\n",
        "        self.rand_utt = [tokenizer.mask_token_id]*(max_utt_len-1) + [tokenizer.sep_token_id] # update during loading\n",
        "\n",
        "        # a cache to store context and response that are longer than min_num_utts\n",
        "        self.cache = [[tokenizer.mask_token_id]*max_utt_len]*max_num_utts, [tokenizer.mask_token_id]*max_utt_len\n",
        "\n",
        "        self.perm_list = [list(itertools.permutations(range(L))) for L in range(1, max_num_utts+1)]\n",
        "        print(\"loading data...\")\n",
        "        table = tables.open_file(file_path)\n",
        "        self.contexts = table.get_node('/column')[:].astype(np.long)\n",
        "        #self.knowlege = table.get_node('/knowledge')[:].astype(np.long)\n",
        "        self.index = table.get_node('/indices')[:]\n",
        "        self.data_len = self.index.shape[0]\n",
        "        print(\"{} entries\".format(self.data_len))\n",
        "\n",
        "    def __getitem__(self, offset):\n",
        "        index = self.index[offset]\n",
        "        pos_utt, ctx_len, res_len,  = index['pos_utt'], index['ctx_len'], index['res_len']\n",
        "        #pos_knowl, knowl_len = index['pos_knowl'], index['knowl_len']\n",
        "\n",
        "        ctx_len = min(ctx_len, self.block_size) if self.block_size>-1 else ctx_len# trunck too long context\n",
        "\n",
        "        ctx_arr=self.contexts[pos_utt-ctx_len:pos_utt].tolist()\n",
        "        res_arr=self.contexts[pos_utt:pos_utt+res_len].tolist()\n",
        "        #knowl_arr = self.knowledge[pos_knowl:pos_knowl+knowl_len].tolist()\n",
        "\n",
        "        ## split context array into utterances\n",
        "        context = []\n",
        "        tmp_utt = []\n",
        "        for i, tok in enumerate(ctx_arr):\n",
        "            tmp_utt.append(ctx_arr[i])\n",
        "            if tok == self.tokenizer.sep_token_id:\n",
        "                floor = tmp_utt[0]\n",
        "                tmp_utt = tmp_utt[1:]\n",
        "                utt_len = min(len(tmp_utt), self.max_utt_len) # floor is not counted in the utt length\n",
        "                utt = tmp_utt[:utt_len]\n",
        "                context.append(utt)  # append utt to context\n",
        "                tmp_utt=[]  # reset tmp utt\n",
        "        response = res_arr[1:] # ignore cls token at the begining\n",
        "        res_len = min(len(response),self.max_utt_len)\n",
        "        response = response[:res_len-1] + [self.tokenizer.sep_token_id]\n",
        "\n",
        "        '''\n",
        "        knowledge = knowl_arr[:]\n",
        "        knowl_len = min(len(knowledge),self.max_utt_len)\n",
        "        knowledge = knowledge[:knowl_len-1] + [self.tokenizer.sep_token_id]\n",
        "        '''\n",
        "\n",
        "        # balancing by removing short contexts\n",
        " #       if len(context)< self.min_num_utts:\n",
        " #           context, response = self.cache\n",
        " #       else:\n",
        " #           self.cache = deepcopy(context), deepcopy(response)\n",
        "        # end balancing\n",
        "\n",
        "        num_utts = min(len(context), self.max_num_utts)\n",
        "        context = context[-num_utts:]\n",
        "\n",
        "        return context, response #, knowlege\n",
        "\n",
        "    def list2array(self, L, d1_len, d2_len=0, d3_len=0, dtype=np.long, pad_idx=0):\n",
        "        '''  convert a list to an array or matrix  '''\n",
        "        def list_dim(a):\n",
        "            if type(a)!=list: return 0\n",
        "            elif len(a)==0: return 1\n",
        "            else: return list_dim(a[0])+1\n",
        "\n",
        "        if type(L) is not list:\n",
        "            print(\"requires a (nested) list as input\")\n",
        "            return None\n",
        "\n",
        "        if list_dim(L)==0: return L\n",
        "        elif list_dim(L) == 1:\n",
        "            arr = np.zeros(d1_len, dtype=dtype)+pad_idx\n",
        "            for i, v in enumerate(L): arr[i] = v\n",
        "            return arr\n",
        "        elif list_dim(L) == 2:\n",
        "            arr = np.zeros((d2_len, d1_len), dtype=dtype)+pad_idx\n",
        "            for i, row in enumerate(L):\n",
        "                for j, v in enumerate(row):\n",
        "                    arr[i][j] = v\n",
        "            return arr\n",
        "        elif list_dim(L) == 3:\n",
        "            arr = np.zeros((d3_len, d2_len, d1_len), dtype=dtype)+pad_idx\n",
        "            for k, group in enumerate(L):\n",
        "                for i, row in enumerate(group):\n",
        "                    for j, v in enumerate(row):\n",
        "                        arr[k][i][j] = v\n",
        "            return arr\n",
        "        else:\n",
        "            print('error: the list to be converted cannot have a dimenson exceeding 3')\n",
        "\n",
        "    def mask_words(self, utt):\n",
        "        output_label = []\n",
        "        tokens = [tok for tok in utt]\n",
        "        for i, token in enumerate(utt):\n",
        "            prob = random.random()\n",
        "            if prob < 0.15 and not token in [self.tokenizer.pad_token_id, self.tokenizer.sep_token_id, self.tokenizer.cls_token_id]:\n",
        "                prob /= 0.15\n",
        "                if prob < 0.8:\n",
        "                    tokens[i] = self.tokenizer.mask_token_id   # 80% randomly change token to mask token\n",
        "                elif prob < 0.9:\n",
        "                    tokens[i] = random.randint(5, len(self.tokenizer)-5)# 10% randomly change token to random token\n",
        "                output_label.append(token)\n",
        "            else:\n",
        "                output_label.append(-100)\n",
        "        return tokens, output_label\n",
        "\n",
        "\n",
        "    def swap_utt(self, utt):\n",
        "        utt_sop_label = 0 if random.random()>0.6 or len(utt)<5 else 1\n",
        "        tokens = [tok for tok in utt]\n",
        "        utt_len = len(tokens)\n",
        "        if utt_len == self.max_utt_len: # if utt has reached the maximum length, then remove the last token because we will add a new sep token\n",
        "            tokens = tokens[:-2]+ [self.tokenizer.sep_token_id]\n",
        "            utt_len-=1\n",
        "        sep_pos = random.randrange(2, utt_len-1) # seperate position where tokens to the right are random or coherent contexts\n",
        "\n",
        "        # new utt\n",
        "        L_utt, R_utt = tokens[1:sep_pos]+[self.tokenizer.sep_token_id], tokens[sep_pos:]\n",
        "        swaped_utt = L_utt + R_utt if utt_sop_label ==0 else R_utt + L_utt\n",
        "        swaped_utt = [self.tokenizer.cls_token_id] + swaped_utt\n",
        "        utt_attn_mask = [1]*len(swaped_utt)\n",
        "        # segment_ids\n",
        "        utt_segment_ids = [0]*(sep_pos+1)+[1]*(utt_len-sep_pos) if utt_sop_label == 0 else [0]*(utt_len-sep_pos+1)+[1]*(sep_pos)\n",
        "\n",
        "        return swaped_utt, utt_attn_mask, utt_segment_ids, utt_sop_label\n",
        "\n",
        "    def mask_context(self, context):\n",
        "        def is_special_utt(utt):\n",
        "            return len(utt)==3 and utt[1] in [self.tokenizer.mask_token_id, self.tokenizer.sep_token_id, self.tokenizer.cls_token_id]\n",
        "\n",
        "        utts = [utt for utt in context]\n",
        "        lm_label = [[-100]*len(utt) for utt in context]\n",
        "        context_len = len(context)\n",
        "        assert context_len>1, 'a context to be masked should have at least 2 utterances'\n",
        "\n",
        "        mlm_probs = [0.0, 0.1, 0.4, 0.7, 0.8, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
        "        mlm_prob = mlm_probs[context_len-1]\n",
        "\n",
        "        prob = random.random()\n",
        "        if prob < mlm_prob:\n",
        "            i = random.randrange(context_len)\n",
        "            while is_special_utt(utts[i]):\n",
        "                i = random.randrange(context_len)\n",
        "            utt = utts[i]\n",
        "            prob = prob/mlm_prob\n",
        "            if prob < 0.8: # 80% randomly change utt to mask utt\n",
        "                utts[i] = [self.tokenizer.cls_token_id, self.tokenizer.mask_token_id, self.tokenizer.sep_token_id]\n",
        "            elif prob < 0.9: # 10% randomly change utt to a random utt\n",
        "                utts[i] = deepcopy(self.rand_utt)\n",
        "            lm_label[i]= deepcopy(utt)\n",
        "            #assert len(utts[i]) == len(lm_label[i]), \"the size of the lm label is different to that of the masked utterance\"\n",
        "            self.rand_utt = deepcopy(utt) # update random utt\n",
        "        return utts, lm_label\n",
        "\n",
        "    def shuf_ctx(self, context):\n",
        "        perm_label = 0\n",
        "        num_utts = len(context)\n",
        "        if num_utts==1:\n",
        "            return context, perm_label, [0]\n",
        "        for i in range(num_utts-1): perm_label += len(self.perm_list[i])\n",
        "        perm_id = int(random.random()*len(self.perm_list[num_utts-1]))\n",
        "        perm_label += perm_id\n",
        "        ctx_position_ids = self.perm_list[num_utts-1][perm_id]\n",
        "        # new context\n",
        "        shuf_context = [context[i] for i in ctx_position_ids]\n",
        "        return shuf_context, perm_label, ctx_position_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3eR600j1S7",
        "outputId": "07dc7df3-10a6-41cf-fe37-0b64eb53ba18"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-63744af71d14>:99: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  def list2array(self, L, d1_len, d2_len=0, d3_len=0, dtype=np.long, pad_idx=0):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT 토크나이저 불러오기\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWvBuVPh-Xsv",
        "outputId": "3a7cc60a-b92d-4bbf-feab-94bef6eeb500"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DialogBert masking\n",
        "'''\n",
        "import pandas as pd\n",
        "import torch\n",
        "import math\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_dict(filename):\n",
        "    import json\n",
        "    with open(filename, \"r\") as f:\n",
        "        return json.loads(f.readline())\n",
        "\n",
        "def load_vecs(fin):\n",
        "    import tables\n",
        "    import numpy as np\n",
        "    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n",
        "    h5f = tables.open_file(fin)\n",
        "    h5vecs = h5f.root.vecs\n",
        "    vecs = np.zeros(shape=h5vecs.shape, dtype=h5vecs.dtype)\n",
        "    vecs[:] = h5vecs[:]\n",
        "    h5f.close()\n",
        "    return vecs\n",
        "\n",
        "def save_vecs(vecs, fout):\n",
        "    import tables\n",
        "    fvec = tables.open_file(fout, 'w')\n",
        "    atom = tables.Atom.from_dtype(vecs.dtype)\n",
        "    filters = tables.Filters(complib='blosc', complevel=5)\n",
        "    ds = fvec.create_carray(fvec.root, 'vecs', atom, vecs.shape, filters=filters)\n",
        "    ds[:] = vecs\n",
        "    print('done')\n",
        "    fvec.close()\n",
        "\n",
        "class DialogTransformerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, tokenizer, min_num_utts=2, max_num_utts=15, max_utt_len=50, block_size=512,\n",
        "                 utt_masklm=False, utt_sop=False, context_shuf=False, context_masklm=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.min_num_utts = min_num_utts\n",
        "        self.max_num_utts = max_num_utts\n",
        "        self.max_utt_len = max_utt_len\n",
        "        self.block_size = block_size\n",
        "        self.utt_masklm = utt_masklm\n",
        "        self.utt_sop = utt_sop\n",
        "        self.context_shuf = context_shuf\n",
        "        self.context_masklm = context_masklm\n",
        "\n",
        "        print(\"Loading data...\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        self.contexts = df['corrected_column'].values  # Adjust according to your CSV column names\n",
        "        # self.knowledge = df['knowledge'].values  # Uncomment if you have this column\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.contexts[idx]\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_utt_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_dict['input_ids'].squeeze()\n",
        "        attention_mask = encoded_dict['attention_mask'].squeeze()\n",
        "\n",
        "        if self.utt_masklm or self.context_masklm:\n",
        "            input_ids, labels = self.mask_tokens(input_ids)\n",
        "        else:\n",
        "            labels = input_ids\n",
        "\n",
        "        return input_ids, attention_mask, labels, torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.contexts[idx]\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_utt_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_dict['input_ids'].squeeze()\n",
        "        attention_mask = encoded_dict['attention_mask'].squeeze()\n",
        "\n",
        "        # You need to implement other outputs as required\n",
        "        return input_ids, attention_mask, torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "\n",
        "    def mask_tokens(self, input_ids):\n",
        "        labels = input_ids.clone()\n",
        "        probability_matrix = torch.full(labels.shape, 0.15)\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, replace masked input tokens with [MASK]\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        input_ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "        # 10% of the time, replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "        input_ids[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # The rest 10% of the time, keep the masked input tokens unchanged\n",
        "\n",
        "        return input_ids, labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_dir = '/content/drive/MyDrive/NLP/Dataset/240527/'\n",
        "    VALID_FILE = input_dir + '[haein]dataset_cleansing+koSpacing+drop_duplicate_240527.csv'\n",
        "\n",
        "    task = 'test_ctx'  # or 'test_utt'\n",
        "\n",
        "    if task == 'test_utt':\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, utt_masklm=True, utt_sop=True)\n",
        "    elif task == 'test_ctx':\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, context_shuf=True, context_masklm=False)\n",
        "    else:\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "    #print(train_loader.head())\n",
        "    eval_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=1)\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained('skt/kobert-base-v1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    learning_rate = 5e-5\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = CrossEntropyLoss()\n",
        "\n",
        "    epochs = 1\n",
        "    history = {'train_loss': [], 'eval_loss': [], 'train_accuracy': [], 'eval_accuracy': [], 'train_perplexity': [], 'eval_perplexity': []}\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            optimizer.zero_grad()\n",
        "            context, context_attn_mask, _, _, _, _, _ = batch\n",
        "\n",
        "            input_ids = context.to(device)\n",
        "            attention_mask = context_attn_mask.to(device)\n",
        "            labels = input_ids  # Assuming labels are the same as input_ids for MLM\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_predictions += labels.numel()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = correct_predictions / total_predictions\n",
        "        train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_accuracy'].append(train_accuracy)\n",
        "        history['train_perplexity'].append(train_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        eval_predictions = []\n",
        "        eval_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(eval_loader, desc=f\"Evaluation Epoch {epoch+1}\"):\n",
        "                context, context_attn_mask, _, _, _, _, _ = batch\n",
        "\n",
        "                input_ids = context.to(device)\n",
        "                attention_mask = context_attn_mask.to(device)\n",
        "                labels = input_ids  # Assuming labels are the same as input_ids for MLM\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                eval_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                correct_predictions += (preds == labels).sum().item()\n",
        "                total_predictions += labels.numel()\n",
        "\n",
        "                eval_predictions.extend(preds.cpu().numpy())\n",
        "                eval_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        eval_accuracy = correct_predictions / total_predictions\n",
        "        eval_perplexity = math.exp(avg_eval_loss)\n",
        "\n",
        "        history['eval_loss'].append(avg_eval_loss)\n",
        "        history['eval_accuracy'].append(eval_accuracy)\n",
        "        history['eval_perplexity'].append(eval_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Eval Loss: {avg_eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}, Eval Perplexity: {eval_perplexity:.4f}\")\n",
        "\n",
        "        all_predictions.extend(eval_predictions)\n",
        "        all_targets.extend(eval_targets)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "et2T0ycR_ThZ",
        "outputId": "242270c4-a1a4-4a16-f22d-1891edc6c317"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport pandas as pd\\nimport torch\\nimport math\\nfrom transformers import BertTokenizer, BertForMaskedLM, AdamW\\nfrom torch.nn import CrossEntropyLoss\\nfrom tqdm import tqdm\\n\\ndef load_dict(filename):\\n    import json\\n    with open(filename, \"r\") as f:\\n        return json.loads(f.readline())\\n\\ndef load_vecs(fin):\\n    import tables\\n    import numpy as np\\n    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\\n    h5f = tables.open_file(fin)\\n    h5vecs = h5f.root.vecs\\n    vecs = np.zeros(shape=h5vecs.shape, dtype=h5vecs.dtype)\\n    vecs[:] = h5vecs[:]\\n    h5f.close()\\n    return vecs\\n\\ndef save_vecs(vecs, fout):\\n    import tables\\n    fvec = tables.open_file(fout, \\'w\\')\\n    atom = tables.Atom.from_dtype(vecs.dtype)\\n    filters = tables.Filters(complib=\\'blosc\\', complevel=5)\\n    ds = fvec.create_carray(fvec.root, \\'vecs\\', atom, vecs.shape, filters=filters)\\n    ds[:] = vecs\\n    print(\\'done\\')\\n    fvec.close()\\n\\nclass DialogTransformerDataset(torch.utils.data.Dataset):\\n    def __init__(self, file_path, tokenizer, min_num_utts=2, max_num_utts=15, max_utt_len=50, block_size=512,\\n                 utt_masklm=False, utt_sop=False, context_shuf=False, context_masklm=False):\\n        self.tokenizer = tokenizer\\n        self.min_num_utts = min_num_utts\\n        self.max_num_utts = max_num_utts\\n        self.max_utt_len = max_utt_len\\n        self.block_size = block_size\\n        self.utt_masklm = utt_masklm\\n        self.utt_sop = utt_sop\\n        self.context_shuf = context_shuf\\n        self.context_masklm = context_masklm\\n        \\n        print(\"Loading data...\")\\n        df = pd.read_csv(file_path)\\n        self.contexts = df[\\'corrected_column\\'].values  # Adjust according to your CSV column names\\n        # self.knowledge = df[\\'knowledge\\'].values  # Uncomment if you have this column\\n\\n    def __len__(self):\\n        return len(self.contexts)\\n\\n    def __getitem__(self, idx):\\n        context = self.contexts[idx]\\n        encoded_dict = self.tokenizer.encode_plus(\\n            context,\\n            add_special_tokens=True,\\n            max_length=self.max_utt_len,\\n            pad_to_max_length=True,\\n            truncation=True,\\n            return_attention_mask=True,\\n            return_tensors=\\'pt\\',\\n        )\\n\\n        input_ids = encoded_dict[\\'input_ids\\'].squeeze()\\n        attention_mask = encoded_dict[\\'attention_mask\\'].squeeze()\\n\\n        if self.utt_masklm or self.context_masklm:\\n            input_ids, labels = self.mask_tokens(input_ids)\\n        else:\\n            labels = input_ids\\n\\n        return input_ids, attention_mask, labels, torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\\n\\n\\n    def __getitem__(self, idx):\\n        context = self.contexts[idx]\\n        encoded_dict = self.tokenizer.encode_plus(\\n            context,\\n            add_special_tokens=True,\\n            max_length=self.max_utt_len,\\n            pad_to_max_length=True,\\n            truncation=True,\\n            return_attention_mask=True,\\n            return_tensors=\\'pt\\',\\n        )\\n\\n        input_ids = encoded_dict[\\'input_ids\\'].squeeze()\\n        attention_mask = encoded_dict[\\'attention_mask\\'].squeeze()\\n        \\n        # You need to implement other outputs as required\\n        return input_ids, attention_mask, torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\\n  \\n    def mask_tokens(self, input_ids):\\n        labels = input_ids.clone()\\n        probability_matrix = torch.full(labels.shape, 0.15)\\n        special_tokens_mask = [\\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\\n        ]\\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\\n        masked_indices = torch.bernoulli(probability_matrix).bool()\\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\\n\\n        # 80% of the time, replace masked input tokens with [MASK]\\n        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\\n        input_ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\\n\\n        # 10% of the time, replace masked input tokens with random word\\n        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\\n        input_ids[indices_random] = random_words[indices_random]\\n\\n        # The rest 10% of the time, keep the masked input tokens unchanged\\n\\n        return input_ids, labels\\n\\nif __name__ == \\'__main__\\':\\n    input_dir = \\'/content/drive/MyDrive/NLP/Dataset/240527/\\'\\n    VALID_FILE = input_dir + \\'[haein]dataset_cleansing+koSpacing+drop_duplicate_240527.csv\\'\\n\\n    task = \\'test_ctx\\'  # or \\'test_utt\\'\\n\\n    if task == \\'test_utt\\':\\n        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, utt_masklm=True, utt_sop=True)\\n    elif task == \\'test_ctx\\':\\n        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, context_shuf=True, context_masklm=False)\\n    else:\\n        dataset = DialogTransformerDataset(VALID_FILE, tokenizer)\\n\\n    train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=1)\\n    #print(train_loader.head())\\n    eval_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=1)\\n\\n    model = BertForMaskedLM.from_pretrained(\\'skt/kobert-base-v1\\')\\n    device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n    model.to(device)\\n\\n    learning_rate = 5e-5\\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\\n    criterion = CrossEntropyLoss()\\n\\n    epochs = 1\\n    history = {\\'train_loss\\': [], \\'eval_loss\\': [], \\'train_accuracy\\': [], \\'eval_accuracy\\': [], \\'train_perplexity\\': [], \\'eval_perplexity\\': []}\\n    all_predictions = []\\n    all_targets = []\\n\\n    for epoch in range(epochs):\\n        model.train()\\n        running_loss = 0.0\\n        correct_predictions = 0\\n        total_predictions = 0\\n\\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\\n            optimizer.zero_grad()\\n            context, context_attn_mask, _, _, _, _, _ = batch\\n            \\n            input_ids = context.to(device)\\n            attention_mask = context_attn_mask.to(device)\\n            labels = input_ids  # Assuming labels are the same as input_ids for MLM\\n\\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\\n            loss = outputs.loss\\n            logits = outputs.logits\\n\\n            loss.backward()\\n            optimizer.step()\\n\\n            running_loss += loss.item()\\n            preds = torch.argmax(logits, dim=-1)\\n            correct_predictions += (preds == labels).sum().item()\\n            total_predictions += labels.numel()\\n\\n        avg_train_loss = running_loss / len(train_loader)\\n        train_accuracy = correct_predictions / total_predictions\\n        train_perplexity = math.exp(avg_train_loss)\\n\\n        history[\\'train_loss\\'].append(avg_train_loss)\\n        history[\\'train_accuracy\\'].append(train_accuracy)\\n        history[\\'train_perplexity\\'].append(train_perplexity)\\n\\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\\n\\n        model.eval()\\n        eval_loss = 0.0\\n        correct_predictions = 0\\n        total_predictions = 0\\n\\n        eval_predictions = []\\n        eval_targets = []\\n\\n        with torch.no_grad():\\n            for batch in tqdm(eval_loader, desc=f\"Evaluation Epoch {epoch+1}\"):\\n                context, context_attn_mask, _, _, _, _, _ = batch\\n\\n                input_ids = context.to(device)\\n                attention_mask = context_attn_mask.to(device)\\n                labels = input_ids  # Assuming labels are the same as input_ids for MLM\\n\\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\\n                loss = outputs.loss\\n                logits = outputs.logits\\n\\n                eval_loss += loss.item()\\n                preds = torch.argmax(logits, dim=-1)\\n                correct_predictions += (preds == labels).sum().item()\\n                total_predictions += labels.numel()\\n\\n                eval_predictions.extend(preds.cpu().numpy())\\n                eval_targets.extend(labels.cpu().numpy())\\n\\n        avg_eval_loss = eval_loss / len(eval_loader)\\n        eval_accuracy = correct_predictions / total_predictions\\n        eval_perplexity = math.exp(avg_eval_loss)\\n\\n        history[\\'eval_loss\\'].append(avg_eval_loss)\\n        history[\\'eval_accuracy\\'].append(eval_accuracy)\\n        history[\\'eval_perplexity\\'].append(eval_perplexity)\\n\\n        print(f\"Epoch {epoch+1}/{epochs}, Eval Loss: {avg_eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}, Eval Perplexity: {eval_perplexity:.4f}\")\\n\\n        all_predictions.extend(eval_predictions)\\n        all_targets.extend(eval_targets)\\n\\n    print(\"Training complete.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DialogBERT & KoBERT Masking\n",
        "import pandas as pd\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_dict(filename):\n",
        "    import json\n",
        "    with open(filename, \"r\") as f:\n",
        "        return json.loads(f.readline())\n",
        "\n",
        "def load_vecs(fin):\n",
        "    import tables\n",
        "    import numpy as np\n",
        "    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n",
        "    h5f = tables.open_file(fin)\n",
        "    h5vecs = h5f.root.vecs\n",
        "    vecs = np.zeros(shape=h5vecs.shape, dtype=h5vecs.dtype)\n",
        "    vecs[:] = h5vecs[:]\n",
        "    h5f.close()\n",
        "    return vecs\n",
        "\n",
        "def save_vecs(vecs, fout):\n",
        "    import tables\n",
        "    fvec = tables.open_file(fout, 'w')\n",
        "    atom = tables.Atom.from_dtype(vecs.dtype)\n",
        "    filters = tables.Filters(complib='blosc', complevel=5)\n",
        "    ds = fvec.create_carray(fvec.root, 'vecs', atom, vecs.shape, filters=filters)\n",
        "    ds[:] = vecs\n",
        "    print('done')\n",
        "    fvec.close()\n",
        "\n",
        "class DialogTransformerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, tokenizer, min_num_utts=2, max_num_utts=15, max_utt_len=50, block_size=512,\n",
        "                 utt_masklm=False, utt_sop=False, context_shuf=False, context_masklm=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.min_num_utts = min_num_utts\n",
        "        self.max_num_utts = max_num_utts\n",
        "        self.max_utt_len = max_utt_len\n",
        "        self.block_size = block_size\n",
        "        self.utt_masklm = utt_masklm\n",
        "        self.utt_sop = utt_sop\n",
        "        self.context_shuf = context_shuf\n",
        "        self.context_masklm = context_masklm\n",
        "\n",
        "        print(\"Loading data...\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        self.contexts = df['corrected_column'].values  # Adjust according to your CSV column names\n",
        "        # self.knowledge = df['knowledge'].values  # Uncomment if you have this column\n",
        "\n",
        "        self.masked_indices = set(random.sample(range(len(self.contexts)), len(self.contexts) // 10))  # Mask 10% of the data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.contexts[idx]\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_utt_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_dict['input_ids'].squeeze()\n",
        "        input_ids_nomask = input_ids.clone()\n",
        "        attention_mask = encoded_dict['attention_mask'].squeeze()\n",
        "        target_ids = input_ids.clone()  # Target IDs are the original input IDs\n",
        "\n",
        "        # Masking\n",
        "        if idx in self.masked_indices:\n",
        "            for i in range(len(input_ids)):\n",
        "                if input_ids[i] not in [\n",
        "                    self.tokenizer.cls_token_id,\n",
        "                    self.tokenizer.sep_token_id,\n",
        "                    self.tokenizer.pad_token_id\n",
        "                ]:\n",
        "                    rand = random.random()\n",
        "                    if rand < 0.8:\n",
        "                        # 80% chance to replace with [MASK] token\n",
        "                        input_ids[i] = self.tokenizer.mask_token_id\n",
        "                    elif rand < 0.9:\n",
        "                        # 10% chance to replace with a random token\n",
        "                        input_ids[i] = random.randint(0, self.tokenizer.vocab_size - 1)\n",
        "                    # Remaining 10% keep the original token\n",
        "                else:\n",
        "                    target_ids[i] = -100  # Ignore non-masked tokens during loss computation\n",
        "\n",
        "        return {\n",
        "            'input_ids_nomask': input_ids_nomask,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': target_ids  # Use masked values as labels\n",
        "        }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_dir = '/content/drive/MyDrive/NLP/Dataset/240527/'\n",
        "    VALID_FILE = input_dir + '[haein]dataset_cleansing+koSpacing+drop_duplicate_240527.csv'\n",
        "\n",
        "    #tokenizer = BertTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "    task = 'test_ctx'  # or 'test_utt'\n",
        "\n",
        "    if task == 'test_utt':\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, utt_masklm=True, utt_sop=True)\n",
        "    elif task == 'test_ctx':\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer, context_shuf=True, context_masklm=False)\n",
        "    else:\n",
        "        dataset = DialogTransformerDataset(VALID_FILE, tokenizer)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "    eval_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=1)\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained('skt/kobert-base-v1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    learning_rate = 5e-5\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = CrossEntropyLoss()\n",
        "\n",
        "    epochs = 1\n",
        "    history = {'train_loss': [], 'eval_loss': [], 'train_accuracy': [], 'eval_accuracy': [], 'train_perplexity': [], 'eval_perplexity': []}\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids_nomask = batch['input_ids_nomask'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)  # Assuming labels are the same as input_ids for MLM\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_predictions += labels.numel()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = correct_predictions / total_predictions\n",
        "        train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_accuracy'].append(train_accuracy)\n",
        "        history['train_perplexity'].append(train_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        eval_predictions = []\n",
        "        eval_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(eval_loader, desc=f\"Evaluation Epoch {epoch+1}\"):\n",
        "                input_ids_nomask = batch['input_ids_nomask'].to(device)\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)  # Assuming labels are the same as input_ids for MLM\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                eval_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                correct_predictions += (preds == labels).sum().item()\n",
        "                total_predictions += labels.numel()\n",
        "\n",
        "                eval_predictions.extend(preds.cpu().numpy())\n",
        "                eval_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        eval_accuracy = correct_predictions / total_predictions\n",
        "        eval_perplexity = math.exp(avg_eval_loss)\n",
        "\n",
        "        history['eval_loss'].append(avg_eval_loss)\n",
        "        history['eval_accuracy'].append(eval_accuracy)\n",
        "        history['eval_perplexity'].append(eval_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Eval Loss: {avg_eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}, Eval Perplexity: {eval_perplexity:.4f}\")\n",
        "\n",
        "        all_predictions.extend(eval_predictions)\n",
        "        all_targets.extend(eval_targets)\n",
        "\n",
        "    print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OupbwcWXKdCK",
        "outputId": "e0ebbc01-0682-4cdb-ac56-c26e6ed1343f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Training Epoch 1: 100%|██████████| 1502/1502 [07:09<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 0.6938, Train Accuracy: 0.8623, Train Perplexity: 2.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation Epoch 1: 100%|██████████| 1502/1502 [02:13<00:00, 11.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Eval Loss: 0.4727, Eval Accuracy: 0.8913, Eval Perplexity: 1.6044\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_targets = [[token for token in target if token != -100] for target in all_targets]"
      ],
      "metadata": {
        "id": "J8HLN_YcOrEp"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding predictions and targets\n",
        "decoded_predictions = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(pred)) for pred in all_predictions]"
      ],
      "metadata": {
        "id": "jEw68nY1nK4c"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_predictions[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD5o3gl9NsR5",
        "outputId": "a2dee997-e2de-4355-c247-4a994b648a3e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD] 안녕하세요. 저는 요즘 정말 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_targets = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(target)) for target in valid_targets]"
      ],
      "metadata": {
        "id": "kLZ-4THsNivo"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display some decoded predictions and targets\n",
        "for i in range(100):\n",
        "    print(f\"Prediction {i+1}: {decoded_predictions[i]}\")\n",
        "    print(f\"Target {i+1}: {decoded_targets[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKJKZGqRNnde",
        "outputId": "1167831c-0f80-4d61-9d11-b479a1264636"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction 1: [PAD] 네, 안녕하세요.요즘 정말 좋은 일이 많이 있었거든요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 1: [CLS] 네, 안녕하세요.요즘 정말 좋은 일이 많이 있었거든요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 2: [PAD] 네, 맞아요.정말 행복한 시기를 보내고 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 2: [CLS] 네, 맞아요.정말 행복한 시기를 보내고 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 3: [PAD] 그렇죠. 어려운 시기를 견뎌내고 이렇게 행복한 날들을 맞이할 수 있어 감사해요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 3: [CLS] 그렇죠. 어려운 시기를 견뎌내고 이렇게 행복한 날들을 맞이할 수 있어 감사해요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 4: [PAD] 맞아요.정말 기분 좋은 일이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 4: [CLS] 맞아요.정말 기분 좋은 일이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 5: [PAD] 네, 감사합니다. 상담해주셔서 정말 고맙습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 5: [CLS] 네, 감사합니다. 상담해주셔서 정말 고맙습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 6: [PAD] 안녕하세요. 저는 요즘 정말 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 6: [CLS] 안녕하세요. 저는 요즘 정말 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 7: [PAD] 네, 정말 힘들어요. 친구들도 없고 가족들과도 자주 싸워요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 7: [CLS] 네, 정말 힘들어요. 친구들도 없고 가족들과도 자주 싸워요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 8: [PAD] 정말 도와줄 사람이 없어요. 부모님도 제 마음을 이해해 주시지 않고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 8: [CLS] 정말 도와줄 사람이 없어요. 부모님도 제 마음을 이해해 주시지 않고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 9: [PAD] 그렇지만 저는 정말 자살 외에는 다른 선택지가 없는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 9: [CLS] 그렇지만 저는 정말 자살 외에는 다른 선택지가 없는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 10: [PAD] 정말 그렇게 생각할 수 있을까요?제 앞날은 너무 암담해 보여요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 10: [CLS] 정말 그렇게 생각할 수 있을까요?제 앞날은 너무 암담해 보여요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 11: [PAD] 안녕하세요.제가 일하면서 공부하느라 정말 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 11: [CLS] 안녕하세요.제가 일하면서 공부하느라 정말 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 12: [PAD] 네, 이렇게 살면서 행복해질 수 있을까 의문이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 12: [CLS] 네, 이렇게 살면서 행복해질 수 있을까 의문이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 13: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 13: 그렇지만 저는 자살 외에는 다른 선택지가 없는 것 같아요.\n",
            "\n",
            "Prediction 14: [PAD] 네, 그렇기 때문에 돈에 대한 스트레스와 집착이 심해요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 14: [CLS] 네, 그렇기 때문에 돈에 대한 스트레스와 집착이 심해요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 15: [PAD] 네, 하지만 현실적으로 봤을 때 상황이 나아지긴 쉽지 않을 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 15: [CLS] 네, 하지만 현실적으로 봤을 때 상황이 나아지긴 쉽지 않을 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 16: [PAD] 안녕하세요. 저는 공부에 찌들어 살아가는 학생입니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 16: [CLS] 안녕하세요. 저는 공부에 찌들어 살아가는 학생입니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 17: [PAD] 네, 그렇습니다. 부모님은 제 오빠들이 똑똑해서 저에게는 관심이 없으세요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 17: [CLS] 네, 그렇습니다. 부모님은 제 오빠들이 똑똑해서 저에게는 관심이 없으세요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 18: [PAD] 네, 맞아요. 졸업한 후에는 조금 나아졌지만 슬플 때면 자살 방법을 생각해요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 18: [CLS] 네, 맞아요. 졸업한 후에는 조금 나아졌지만 슬플 때면 자살 방법을 생각해요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 19: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 19: 저도 자살하고 싶지는 않아요. 하지만 이렇게 살아가는 게 너무 힘들어요.\n",
            "\n",
            "Prediction 20: [PAD] 네, 알겠습니다.자살은 극단적인 선택이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 20: [CLS] 네, 알겠습니다.자살은 극단적인 선택이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 21: [PAD] 안녕하세요.말 그대로 아무도 모르게 사라지고 싶습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 21: [CLS] 안녕하세요.말 그대로 아무도 모르게 사라지고 싶습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 22: [PAD] 네, 왜 이런 생각이 드는지 모르겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 22: [CLS] 네, 왜 이런 생각이 드는지 모르겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 23: [PAD] 저는 남한테 피해안 주고 조용히 죽는 방법밖에 없다고 봐요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 23: [CLS] 저는 남한테 피해안 주고 조용히 죽는 방법밖에 없다고 봐요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 24: [PAD] 그렇겠죠? 하지만 저는 지금 이 너무 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 24: [CLS] 그렇겠죠? 하지만 저는 지금 이 너무 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 25: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 25: 네, 알겠습니다. 다른 방법을 찾아봐야겠어요.\n",
            "\n",
            "Prediction 26: [PAD] 안녕하세요.이곳을 찾은 지 얼마 되지 않았는데 벌써 다시 글을 쓰고 있네요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 26: [CLS] 안녕하세요.이곳을 찾은 지 얼마 되지 않았는데 벌써 다시 글을 쓰고 있네요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 27: [PAD] 네, 계속 생각해봐도 전 죽어야 할 사람 같아 자신이 싫어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 27: [CLS] 네, 계속 생각해봐도 전 죽어야 할 사람 같아 자신이 싫어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 28: [PAD] 네, 온라인에서도 정신과를 가보라는 말밖에 못 들었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 28: [CLS] 네, 온라인에서도 정신과를 가보라는 말밖에 못 들었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 29: [PAD] 가위나 식칼, 길가의 나무를 보면 자살상상을 하게 돼요.하지만 저는 겁쟁이라 자살 못할 거예요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 29: [CLS] 가위나 식칼, 길가의 나무를 보면 자살상상을 하게 돼요.하지만 저는 겁쟁이라 자살 못할 거예요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 30: [PAD] 그렇게 살아가면서 주변에 폐만 끼칠 것 같아 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 30: [CLS] 그렇게 살아가면서 주변에 폐만 끼칠 것 같아 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 31: [PAD] 네, 알겠습니다. 힘들지만 계속 버텨나가며 기회를 노려보겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 31: [CLS] 네, 알겠습니다. 힘들지만 계속 버텨나가며 기회를 노려보겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 32: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 32: 안녕하세요. 하고 싶은 것도, 먹고 싶은 것도, 원하는 것도 아무것도 없고 무기력합니다.\n",
            "\n",
            "Prediction 33: [PAD] 숨 쉬는 것조차 제게는 지옥이에요. 눈을 뜨는 순간부터 죽고 싶다는 생각뿐이죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 33: [CLS] 숨 쉬는 것조차 제게는 지옥이에요. 눈을 뜨는 순간부터 죽고 싶다는 생각뿐이죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 34: [PAD] 부족하다는 생각 때문에 그런 건 아닐까요? 뭘 해도 남보다 뒤처진다고 여겨서요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 34: [CLS] 부족하다는 생각 때문에 그런 건 아닐까요? 뭘 해도 남보다 뒤처진다고 여겨서요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 35: ..................................................\n",
            "Target 35: 모든 것에 억지로 의욕을 가질 필요는 없겠지만, 작은 것들을 하나씩 실천해 나가며\n",
            "\n",
            "Prediction 36: [PAD] 안녕하세요. 나름대로 한 시도 쉬지 않고 일하고 인생을 열심히 살아왔는데[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 36: [CLS] 안녕하세요. 나름대로 한 시도 쉬지 않고 일하고 인생을 열심히 살아왔는데[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 37: [PAD] 단 하루라도 맘 편히 고민 없이 자고 싶은데 그럴 방법이 인생을 그만두는 거밖에 없어 보여요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 37: [CLS] 단 하루라도 맘 편히 고민 없이 자고 싶은데 그럴 방법이 인생을 그만두는 거밖에 없어 보여요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 38: [PAD] 네, 당장의 상식으로는 이해되지 않는 상황이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 38: [CLS] 네, 당장의 상식으로는 이해되지 않는 상황이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 39: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 39: 맞아요.제 인생을 위해 노력해왔지만 지금은 공허감에 빠져 있어요.\n",
            "\n",
            "Prediction 40: [PAD] 네, 여러분의 말씀 잘 새기겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 40: [CLS] 네, 여러분의 말씀 잘 새기겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 41: [PAD] 안녕하세요.학교에서는 왕따를 당하고 집에서는 유령 취급을 받아 도망만 다녔어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 41: [CLS] 안녕하세요.학교에서는 왕따를 당하고 집에서는 유령 취급을 받아 도망만 다녔어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 42: [PAD] 가족의 수입 때문에 제가 지원금을 받을 수 있는 것도 없어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 42: [CLS] 가족의 수입 때문에 제가 지원금을 받을 수 있는 것도 없어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 43: [PAD] 그동안 여러 방법을 알아봤지만 가족관계 때문에 제대로 된 지원을 받지 못했어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 43: [CLS] 그동안 여러 방법을 알아봤지만 가족관계 때문에 제대로 된 지원을 받지 못했어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 44: [PAD] 가족과 연락이 닿을 수 있는 방법도 찾아봐야겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 44: [CLS] 가족과 연락이 닿을 수 있는 방법도 찾아봐야겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 45: [PAD] 안녕하세요. 잘 다니던 직장을 때려치우고 전업투자자가 되었는데,[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 45: [CLS] 안녕하세요. 잘 다니던 직장을 때려치우고 전업투자자가 되었는데,[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 46: [PAD] 정말 죄송합니다.답을 알고 있음에도 이런 글을 남겨서요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 46: [CLS] 정말 죄송합니다.답을 알고 있음에도 이런 글을 남겨서요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 47: [PAD] 그렇군요. 서른 초반의 나이에 이런 일을 겪으니 당황스러워요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 47: [CLS] 그렇군요. 서른 초반의 나이에 이런 일을 겪으니 당황스러워요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 48: [PAD] 네, 정주영 회장님 말씀처럼 신용이 가장 중요하다는 걸 명심하겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 48: [CLS] 네, 정주영 회장님 말씀처럼 신용이 가장 중요하다는 걸 명심하겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 49: [PAD] 안녕하세요. 객관적으로 보면 제 상황보다 더한 일을 겪는 사람들이 많다는 걸 알고 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 49: [CLS] 안녕하세요. 객관적으로 보면 제 상황보다 더한 일을 겪는 사람들이 많다는 걸 알고 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 50: [PAD] 맞아요.이 제 모든 게 지겹고 다 놔버리고 싶을 정도로 심신이 많이 지쳐 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 50: [CLS] 맞아요.이 제 모든 게 지겹고 다 놔버리고 싶을 정도로 심신이 많이 지쳐 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 51: [PAD] 그렇죠.원치 않는 생각들이 자주 떠오르지만 누구에게도 털어놓지 못하고 있네요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 51: [CLS] 그렇죠.원치 않는 생각들이 자주 떠오르지만 누구에게도 털어놓지 못하고 있네요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 52: [PAD] 맞아요. 솔직한 감정을 표현하고 주변에 도움을 청하는 것도 중요할 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 52: [CLS] 맞아요. 솔직한 감정을 표현하고 주변에 도움을 청하는 것도 중요할 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 53: [PAD] 안녕하세요. 저번에도 가족의 폭력으로 힘들어 여기에 글을 썼었는데 너무 오래되어서 찾을 수가 없더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 53: [CLS] 안녕하세요. 저번에도 가족의 폭력으로 힘들어 여기에 글을 썼었는데 너무 오래되어서 찾을 수가 없더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 54: [PAD] 네, 맞아요. 중학교 때 친구들과 이야기해보니 제 가족이 너무 달랐다는 걸 알게 됐죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 54: [CLS] 네, 맞아요. 중학교 때 친구들과 이야기해보니 제 가족이 너무 달랐다는 걸 알게 됐죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 55: [PAD] 정말 큰 상처를 받고 울음을 터뜨렸더니 시끄럽다며 또 욕만 하시더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 55: [CLS] 정말 큰 상처를 받고 울음을 터뜨렸더니 시끄럽다며 또 욕만 하시더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 56: [PAD] 구구절절 읽어주셔서 감사합니다.정말 힘들었거든요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 56: [CLS] 구구절절 읽어주셔서 감사합니다.정말 힘들었거든요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 57: [PAD] 부모님과 선생님께서는 성적만 잘 나오면 된다고 생각하시는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 57: [CLS] 부모님과 선생님께서는 성적만 잘 나오면 된다고 생각하시는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 58: [PAD] 감사합니다. 오늘은 힘들지만 내일은 웃을 수 있을 거라 생각합니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 58: [CLS] 감사합니다. 오늘은 힘들지만 내일은 웃을 수 있을 거라 생각합니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 59: [PAD] 안녕하세요.전 중학생인데 큰 고민이 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 59: [CLS] 안녕하세요.전 중학생인데 큰 고민이 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 60: [PAD] 자살 충동도 오고가출한 적도 있어요. 너무 지겨워서였죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 60: [CLS] 자살 충동도 오고가출한 적도 있어요. 너무 지겨워서였죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 61: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 61: 부모님이 싸우실 때면 자살 충동도 오고자해도 해봤어요.\n",
            "\n",
            "Prediction 62: [PAD] 죽고 싶은 마음은 있지만 정작 죽기는 무서워요. 삶에 미련이 없는 것 같아 걱정이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 62: [CLS] 죽고 싶은 마음은 있지만 정작 죽기는 무서워요. 삶에 미련이 없는 것 같아 걱정이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 63: [PAD] 네, 더 이상 제 몸을 괴롭히지 않겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 63: [CLS] 네, 더 이상 제 몸을 괴롭히지 않겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 64: [PAD] 안녕하세요.15살 여학생입니다. 초등 5학년 때 조현병, 우울증 판정을 받고 입원한 적이 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 64: [CLS] 안녕하세요.15살 여학생입니다. 초등 5학년 때 조현병, 우울증 판정을 받고 입원한 적이 있어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 65: [PAD] 돈도 없어서 좋아하던 노래방도 못 가고 있어요.사는 게 힘들어서 엄마한테 같이 죽자는 편지를 썼어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 65: [CLS] 돈도 없어서 좋아하던 노래방도 못 가고 있어요.사는 게 힘들어서 엄마한테 같이 죽자는 편지를 썼어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 66: ..................................................\n",
            "Target 66: 엄마랑은 다음 생에 내가 엄마, 엄마가 딸이 되자 고 약속했어요. 그래서 죽는 게 조금은 덜 무서워졌어요.\n",
            "\n",
            "Prediction 67: [PAD] 작년에도 자살 시도를 했다가 경찰에 붙잡혔었죠. 현실이 너무 힘들어서 그런 생각이 계속 드는 거예요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 67: [CLS] 작년에도 자살 시도를 했다가 경찰에 붙잡혔었죠. 현실이 너무 힘들어서 그런 생각이 계속 드는 거예요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 68: [PAD] 네, 여러분 말씀 잘 새기겠습니다. 혼자는 아니라는 걸 잊지 않고 있으면 조금씩 버텨 나갈 수 있을 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 68: [CLS] 네, 여러분 말씀 잘 새기겠습니다. 혼자는 아니라는 걸 잊지 않고 있으면 조금씩 버텨 나갈 수 있을 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 69: [PAD] 우리 가족한테 미안한 것도 없어요. 키워주셔서 고맙긴 한데 이렇게 해서는 안 되죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 69: [CLS] 우리 가족한테 미안한 것도 없어요. 키워주셔서 고맙긴 한데 이렇게 해서는 안 되죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 70: [PAD] 제 몸에 멍든 것도 봐도 상관없다고 하시더라고요.제가 잘못해서 벌로 때린 거니제 잘못이라고 하셔요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 70: [CLS] 제 몸에 멍든 것도 봐도 상관없다고 하시더라고요.제가 잘못해서 벌로 때린 거니제 잘못이라고 하셔요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 71: [PAD] 제가 자살한다고 해도 부모님은 제가 불효자라고만 하실거에요. 아님 혼자 슬픈 척이나 하겠죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 71: [CLS] 제가 자살한다고 해도 부모님은 제가 불효자라고만 하실거에요. 아님 혼자 슬픈 척이나 하겠죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 72: [PAD] 말이 안 통하니 화가 나서 그렇게 된 거죠. 이렇게라도 대화하니 마음이 조금 가벼워지는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 72: [CLS] 말이 안 통하니 화가 나서 그렇게 된 거죠. 이렇게라도 대화하니 마음이 조금 가벼워지는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 73: 요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요요\n",
            "Target 73: 이렇게 상담해주셔서 정말 감사합니다.\n",
            "\n",
            "Prediction 74: [PAD] 안녕하세요. 올해 15살이 되었는데요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 74: [CLS] 안녕하세요. 올해 15살이 되었는데요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 75: [PAD] 친구를 2명 사귄 것 외에는 아무것도 못했어요. 한 친구에게는 너무 집착했죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 75: [CLS] 친구를 2명 사귄 것 외에는 아무것도 못했어요. 한 친구에게는 너무 집착했죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 76: [PAD] 그 친구는 제 SNS를 다 팔로우하더니 집까지 따라와서 너무 무서웠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 76: [CLS] 그 친구는 제 SNS를 다 팔로우하더니 집까지 따라와서 너무 무서웠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 77: [PAD] 너무 힘들어요. 오늘도 죽고 싶은 마음뿐이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 77: [CLS] 너무 힘들어요. 오늘도 죽고 싶은 마음뿐이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 78: [PAD] 네, 좋은 말씀 감사합니다.제마음을 들여다보고 객관적으로 판단할 수 있는 기회가 되겠네요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 78: [CLS] 네, 좋은 말씀 감사합니다.제마음을 들여다보고 객관적으로 판단할 수 있는 기회가 되겠네요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 79: [PAD] 안녕하세요. 죽고 싶어요.밖에서 괜찮은 척하기 싫고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 79: [CLS] 안녕하세요. 죽고 싶어요.밖에서 괜찮은 척하기 싫고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 80: [PAD] 죽고 싶은 마음은 너무 커요. 어떻게든 이 고통에서 벗어나고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 80: [CLS] 죽고 싶은 마음은 너무 커요. 어떻게든 이 고통에서 벗어나고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 81: [PAD] 너무 힘들어서 그런 생각을 하게 되었죠. 살아갈 의미를 잃었다고 생각했어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 81: [CLS] 너무 힘들어서 그런 생각을 하게 되었죠. 살아갈 의미를 잃었다고 생각했어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 82: [PAD] 감사합니다. 저는 오랫동안 엄마와의 관계가 좋지 않았어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 82: [CLS] 감사합니다. 저는 오랫동안 엄마와의 관계가 좋지 않았어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 83: [PAD] 네, 마음의 상처가 너무 컸죠. 자주 자살을 생각했지만 용기가 나지 않더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 83: [CLS] 네, 마음의 상처가 너무 컸죠. 자주 자살을 생각했지만 용기가 나지 않더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 84: [PAD] 네, 감사합니다. 이렇게 상담해주신 덕분에 더는 극단적인 생각을 하지 않게 되었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 84: [CLS] 네, 감사합니다. 이렇게 상담해주신 덕분에 더는 극단적인 생각을 하지 않게 되었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 85: [PAD] 취업이 안 되고 돈이 다 떨어졌어요. 월세와 대출, 휴대폰 요금도 밀렸고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 85: [CLS] 취업이 안 되고 돈이 다 떨어졌어요. 월세와 대출, 휴대폰 요금도 밀렸고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 86: [PAD] 네, 정말 힘든 상황이에요. 하루하루가 너무 고달프죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 86: [CLS] 네, 정말 힘든 상황이에요. 하루하루가 너무 고달프죠.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 87: [PAD] 네, 좋은 말씀 감사합니다. 당장 도움부터 요청해야겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 87: [CLS] 네, 좋은 말씀 감사합니다. 당장 도움부터 요청해야겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 88: [PAD] 막막하고 절망적이었는데 조금은 희망이 생기는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 88: [CLS] 막막하고 절망적이었는데 조금은 희망이 생기는 것 같아요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 89: [PAD] 인사 보복으로 고통받고 있습니다. 인사팀과 감사관실의 편향된 의견만들어 제고충을 외면했어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 89: [CLS] 인사 보복으로 고통받고 있습니다. 인사팀과 감사관실의 편향된 의견만들어 제고충을 외면했어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 90: [PAD] 네, 정말 답답하고 힘들었어요. 아무리 호소해도 제 목소리는 들리지 않더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 90: [CLS] 네, 정말 답답하고 힘들었어요. 아무리 호소해도 제 목소리는 들리지 않더라고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 91: [PAD] 조언정 말 감사드립니다. 살아있는 한 계속 노력하며 해결책을 찾아 나가겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 91: [CLS] 조언정 말 감사드립니다. 살아있는 한 계속 노력하며 해결책을 찾아 나가겠습니다.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 92: [PAD] 엄마랑맨날 싸워요.가 정위탁이나 쉼터에 보내겠다고 하시고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 92: [CLS] 엄마랑맨날 싸워요.가 정위탁이나 쉼터에 보내겠다고 하시고요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 93: [PAD] 네, 엄마랑 싸울 때마다 정말 속상하고 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 93: [CLS] 네, 엄마랑 싸울 때마다 정말 속상하고 힘들어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 94: [PAD] 네, 맞아요. 상담을 통해 제 마음을 털어놓고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 94: [CLS] 네, 맞아요. 상담을 통해 제 마음을 털어놓고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 95: [PAD] 엄마와의 관계 회복을 위해 전문가의 조언을 들어보고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 95: [CLS] 엄마와의 관계 회복을 위해 전문가의 조언을 들어보고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 96: [PAD] 저는 어린 나이부터 자해를 해왔어요.자해를 하면 할수록 더욱 죽고 싶은 마음뿐이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 96: [CLS] 저는 어린 나이부터 자해를 해왔어요.자해를 하면 할수록 더욱 죽고 싶은 마음뿐이에요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 97: [PAD] 이런 생각이 이상하다는 걸 알지만, 정말 길에 지나가는 모든 사람들과 친구, 가족들을 죽이고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 97: [CLS] 이런 생각이 이상하다는 걸 알지만, 정말 길에 지나가는 모든 사람들과 친구, 가족들을 죽이고 싶어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 98: [PAD] 그렇죠. 주변에 말을 들어줄 사람이 없어서 정말 힘들었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 98: [CLS] 그렇죠. 주변에 말을 들어줄 사람이 없어서 정말 힘들었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 99: [PAD] 말씀해 주신 대로 상담을 받아보는 게 좋겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 99: [CLS] 말씀해 주신 대로 상담을 받아보는 게 좋겠어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n",
            "Prediction 100: [PAD] 저는 중학교 1학년 때부터 우울증과 ADHD, 자폐 등의 병을 앓고 있었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "Target 100: [CLS] 저는 중학교 1학년 때부터 우울증과 ADHD, 자폐 등의 병을 앓고 있었어요.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AphY7jS1nvh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loss(cross-entropy)"
      ],
      "metadata": {
        "id": "GtIRHKpH9Epk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 첫번째 배치 확인 작업"
      ],
      "metadata": {
        "id": "wJbZG6zbCMaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hmij0trjVbNd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNqKJHTUMQGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVMCO8IaB5cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAAywgd4MPy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}